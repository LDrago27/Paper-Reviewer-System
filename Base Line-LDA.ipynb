{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter,defaultdict\n",
    "from nltk import word_tokenize\n",
    "from math import log\n",
    "import string\n",
    "import pickle\n",
    "from math import log\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from string import digits\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from scipy.spatial.distance import cosine\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "osname = os.name\n",
    "if osname =='Windows':\n",
    "    sym = \"//\"\n",
    "else:\n",
    "    sym = \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_path=cwd+sym+'nips_reviewer_data'+sym+'reviewers.txt'\n",
    "review_df=pd.read_csv(r_path,sep='\\t',header=None)\n",
    "review_df.columns=['sno','name']\n",
    "review_lt=list(review_df.name)\n",
    "review_sno={val:ind for ind,val in enumerate(review_lt)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1=cwd+sym+'Saved_Items'+sym\n",
    "with open(path1+'dictionary.pkl','rb') as f:\n",
    "    dictionary=pickle.load(f)\n",
    "with open(path1+'tfidf.pkl','rb') as f:\n",
    "    tfidf=pickle.load(f)\n",
    "with open(path1+'ldaModel','rb') as f:\n",
    "    lda_model=pickle.load(f)\n",
    "with open(path1+'paper_vec.pkl','rb') as f:\n",
    "    paper_vec=pickle.load(f)\n",
    "data_df=pd.read_pickle('data_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'groot'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "st = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "\n",
    "def cleantext(text):\n",
    "    \n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    \n",
    "    # remove punctuation\n",
    "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    strip = text.translate(table)\n",
    "    \n",
    "    \n",
    "    # Tokenizer\n",
    "    tokens = word_tokenize(strip)\n",
    "    \n",
    "    # Convert into lower case \n",
    "    proc_text = [w.lower() for w in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    proc_text = [word for word in proc_text if word not in st]\n",
    "    \n",
    "    \n",
    "    #Storing only Lemmmatized words\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemma_text=[lemmatizer.lemmatize(word) for word in proc_text]\n",
    "\n",
    "    return \" \".join(lemma_text)\n",
    "\n",
    "\n",
    "t = \"i am groot2'7\"    \n",
    "cleantext(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53e99f9bb7602d970286e73d</td>\n",
       "      <td>Alan Murray</td>\n",
       "      <td>Adaptive Noisy Neural Computation in Mixed-mod...</td>\n",
       "      <td>We propose a probabilistic neural computation ...</td>\n",
       "      <td>2003</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53e99f9bb7602d9702870afd</td>\n",
       "      <td>Alan Murray</td>\n",
       "      <td>Geography 683 - Introduction to Geographic Ana...</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53e99f9bb7602d9702870cfb</td>\n",
       "      <td>David Forsyth</td>\n",
       "      <td>Achieve What We Want Them To? Interdependence ...</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53e99f9bb7602d9702871a1e</td>\n",
       "      <td>Ingo Steinwart</td>\n",
       "      <td>Fast Rates to Bayes for Kernel Machines</td>\n",
       "      <td>We establish learning rates to the Bayes risk ...</td>\n",
       "      <td>2004</td>\n",
       "      <td>NIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53e99f9bb7602d9702872b1d</td>\n",
       "      <td>David Forsyth</td>\n",
       "      <td>Primitives, Perceptual Organization and Object...</td>\n",
       "      <td>: We argue that any computational theory of o...</td>\n",
       "      <td>1997</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id          author  \\\n",
       "0  53e99f9bb7602d970286e73d     Alan Murray   \n",
       "1  53e99f9bb7602d9702870afd     Alan Murray   \n",
       "2  53e99f9bb7602d9702870cfb   David Forsyth   \n",
       "3  53e99f9bb7602d9702871a1e  Ingo Steinwart   \n",
       "4  53e99f9bb7602d9702872b1d   David Forsyth   \n",
       "\n",
       "                                               title  \\\n",
       "0  Adaptive Noisy Neural Computation in Mixed-mod...   \n",
       "1  Geography 683 - Introduction to Geographic Ana...   \n",
       "2  Achieve What We Want Them To? Interdependence ...   \n",
       "3            Fast Rates to Bayes for Kernel Machines   \n",
       "4  Primitives, Perceptual Organization and Object...   \n",
       "\n",
       "                                            abstract  year venue  \n",
       "0  We propose a probabilistic neural computation ...  2003        \n",
       "1                                                       -1        \n",
       "2                                                       -1        \n",
       "3  We establish learning rates to the Bayes risk ...  2004  NIPS  \n",
       "4   : We argue that any computational theory of o...  1997        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing reviewer Base LDA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vec_blda={}\n",
    "for sno,name in enumerate(review_lt):\n",
    "    samp_df=data_df[data_df['author']==name]\n",
    "    index_lt=samp_df.index\n",
    "    wts=np.zeros([250], dtype = float)\n",
    "    ctr=0\n",
    "    #print(index_lt)\n",
    "    for ind in index_lt:\n",
    "        ctr+=1\n",
    "        temp_wt=np.array(paper_vec[ind])\n",
    "        wts=wts+temp_wt\n",
    "    wts=wts/ctr\n",
    "    reviewer_vec_blda[sno]=wts\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,reviewer_vec_blda):\n",
    "    clean_text=list(word_tokenize(cleantext(text))) \n",
    "    bow_text=dictionary.doc2bow(clean_text)\n",
    "    tfidf_text=tfidf[bow_text]\n",
    "    topic=lda_model[tfidf_text]\n",
    "    \n",
    "    topic_vec=np.zeros(250)\n",
    "    for ind,wt in topic:\n",
    "        topic_vec[ind]=wt\n",
    "        \n",
    "    #print(topic)\n",
    "    \n",
    "    reviewer_scr=[]\n",
    "    index_lt=list(reviewer_vec_blda.keys())\n",
    "    for index in index_lt:\n",
    "        val=1-cosine(topic_vec,reviewer_vec_blda[index])\n",
    "        if np.isnan(val):\n",
    "            val=0\n",
    "        reviewer_scr.append([index+1,val])\n",
    "        \n",
    "    reviewer_scr.sort(key=lambda x: x[1],reverse=True)\n",
    "    \n",
    "    pred=[]\n",
    "    for i in range(10):\n",
    "        pred.append(reviewer_scr[i][0])\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=path1+'Testing'+sym+'trec_judgments.txt'\n",
    "ordinal=[]\n",
    "value=[]\n",
    "judge={}\n",
    "with open(path,'r',encoding=\"utf8\")as f:\n",
    "    for line in f:\n",
    "        ordinal=list(line.split('\\t'))\n",
    "        value=[int(ordinal[2]),int(ordinal[3][:-1])]\n",
    "        if int(ordinal[0][4:]) in judge:\n",
    "            judge[int(ordinal[0][4:])].append(value)\n",
    "        else :\n",
    "            judge[int(ordinal[0][4:])]=[value]\n",
    "            \n",
    "path=path1+'Testing'+sym+'trec_judgments_additional.txt'\n",
    "with open(path,'r',encoding=\"utf8\")as f:\n",
    "    for line in f:\n",
    "        ordinal=list(line.split('\\t'))\n",
    "        value=[int(ordinal[2]),int(ordinal[3][:-1])]\n",
    "        if int(ordinal[0]) in judge:\n",
    "            judge[int(ordinal[0])].append(value)\n",
    "        else :\n",
    "            judge[int(ordinal[0])]=[value]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(res,gnd_tht,k,paper_no):\n",
    "    actual_raw=gnd_tht[paper_no]\n",
    "    #Getting tupules with unique first element\n",
    "    y = np.unique(actual_raw, axis=0)\n",
    "    actual = [] \n",
    "    for i in y:\n",
    "        actual.append(tuple(i))\n",
    "    \n",
    "    actual_auth=[i[0] for i in actual]\n",
    "    rating=[i[1] for i in actual]\n",
    "    n=len(actual)\n",
    "\n",
    "    soft_p=0\n",
    "    for i in range(k):\n",
    "        predi=res[i]\n",
    "        for j in range(n):\n",
    "            if actual_auth[j]==predi:\n",
    "                if rating[j]==3:\n",
    "                    soft_p+=1\n",
    "                elif rating[j]==2:\n",
    "                    soft_p+=1\n",
    "                elif rating[j]==1:\n",
    "                    soft_p+=0\n",
    "    soft_p=soft_p/k\n",
    "    return soft_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1=cwd+sym+'Saved_Items'+sym\n",
    "with open(path1+'ldaModel250','rb') as f:\n",
    "    lda_model=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path1+'reviewer_vec.pkl','rb') as f:\n",
    "    reviewer_vec=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, 0.0318619267886738), (21, 0.03829375773425805), (24, 0.013382444773253697), (50, 0.0541737918341939), (68, 0.012012634819293832), (84, 0.04400811624782172), (93, 0.01008973659961197), (94, 0.01279021598449918), (99, 0.018962355256592095), (114, 0.08104502274873421), (132, 0.03976908611658863), (173, 0.014710042379313922), (195, 0.011430482823601849), (220, 0.06385691997901066), (221, 0.1798951742468994), (244, 0.010535918368839705)]\n",
      "[(22, 0.016797584585994722), (23, 0.018889342510485255), (36, 0.023863361204982593), (60, 0.04773169358500483), (68, 0.021510352799723527), (84, 0.19935809341839522), (101, 0.028674452115883462), (169, 0.02048300031220164), (220, 0.2329717282502984), (221, 0.1061165224415395)]\n",
      "[(44, 0.01607217350184613), (84, 0.13078329648165526), (85, 0.030335848010421253), (93, 0.014211706939301677), (138, 0.10470320100113895), (151, 0.02057914965212648), (152, 0.01930297381677571), (200, 0.018707346581356258), (220, 0.024412555510165454), (221, 0.2227719667622663), (247, 0.019649197658216735)]\n",
      "[(44, 0.014978656987631851), (65, 0.023370994854189888), (70, 0.02527551682235596), (73, 0.03182882327528443), (74, 0.024795227154434298), (114, 0.07295481237088199), (148, 0.0193646305558414), (167, 0.01987099331727825), (174, 0.018372419311401947), (179, 0.12821939299367532), (221, 0.28846868777172163), (228, 0.01764215918087247), (238, 0.017247646627085594)]\n",
      "[(24, 0.013304493082252332), (36, 0.015918934597927897), (37, 0.014305571030762007), (43, 0.018583920672923412), (45, 0.06458571702107259), (74, 0.016100075727690583), (79, 0.014895440683736305), (84, 0.10469821281983598), (121, 0.029146776771435866), (132, 0.05730100702855339), (186, 0.01373430034946507), (216, 0.015550873001754235), (220, 0.06797452784809788), (221, 0.20255344692297064), (231, 0.021001450644634642), (247, 0.026416255652751398), (249, 0.042111604111813916)]\n",
      "[(9, 0.011502580057344643), (18, 0.011996411214877144), (30, 0.015090436219437222), (32, 0.10923891135955736), (42, 0.03961759711877338), (53, 0.047165366038442416), (65, 0.01204129278156084), (74, 0.015246523152442118), (84, 0.038835595489377664), (94, 0.025283535652839483), (143, 0.10741058731695137), (149, 0.018107256426785102), (164, 0.018659871910066243), (221, 0.18721846722937138), (230, 0.018501113855334192), (242, 0.018415640211046098)]\n",
      "[(8, 0.013042627033723445), (23, 0.2390615857175498), (42, 0.020695938889102688), (53, 0.019315529493890017), (84, 0.021672570762794528), (94, 0.01880595857097973), (106, 0.019549815526986536), (118, 0.0690554667531077), (172, 0.014060306262817198), (175, 0.021526494263561638), (177, 0.028679172986856134), (220, 0.025837855002410273), (221, 0.20996244457462715), (236, 0.026603357932424118)]\n",
      "[(12, 0.01919872920326352), (23, 0.013869875934220847), (37, 0.02401324787989116), (38, 0.019430346824969382), (68, 0.02369628265073044), (84, 0.1880135541749653), (109, 0.02121970267509257), (170, 0.06330334684962459), (188, 0.04092906479667216), (220, 0.0384684754844474), (221, 0.26440474115799373)]\n",
      "[(44, 0.012281915057358539), (56, 0.012022700016870813), (68, 0.12396039739804036), (75, 0.013273946998325334), (84, 0.1087419882320864), (85, 0.05077555217247824), (94, 0.014898618953745802), (110, 0.030879241445971893), (119, 0.01275876251017715), (132, 0.07819886759641576), (143, 0.01518339087209983), (176, 0.055521050862958464), (198, 0.045314821653349685), (220, 0.03232624770926757), (221, 0.13372997361114214), (249, 0.013596578524496925)]\n",
      "[(8, 0.08374256448930617), (23, 0.09152914644855575), (29, 0.024169506360468218), (60, 0.01023717563835953), (84, 0.3231675947962229), (102, 0.05437633728730233), (132, 0.018361270989469863), (135, 0.010342835136257073), (138, 0.014057091088323816), (205, 0.020576784449230705), (220, 0.05264976057642692), (228, 0.010165229155064884)]\n",
      "[(1, 0.04580652134419692), (8, 0.04924620233859124), (22, 0.014930561185268419), (23, 0.15647764133833772), (66, 0.03984845207755774), (85, 0.1369480679797388), (98, 0.014909210621087967), (102, 0.05613910175603622), (173, 0.039641276132206034), (182, 0.03240588277759765), (221, 0.15642415603410054), (233, 0.017251193485330572)]\n",
      "[(8, 0.012938566209659753), (11, 0.16868891723895935), (21, 0.012967499422906497), (58, 0.029060183748327573), (80, 0.023490417168822785), (83, 0.013046846794771794), (84, 0.04749861125055541), (85, 0.01629816612093515), (89, 0.019728246853510652), (132, 0.13403233911365786), (135, 0.016617706718375196), (167, 0.014866110662226116), (199, 0.018288219621067935), (220, 0.011621981077222569), (221, 0.20061364055717062)]\n",
      "[(23, 0.08924985071361165), (30, 0.016352135239463154), (62, 0.02605001083824247), (65, 0.025485742277934317), (84, 0.16366760360949176), (103, 0.014307877157415185), (107, 0.02192215837678641), (113, 0.014504073162084448), (117, 0.030098702720309217), (148, 0.017396868237181857), (166, 0.01934580461906865), (171, 0.031045364796273442), (185, 0.05488315098455042), (212, 0.01755194894382198), (214, 0.03059194240925047), (221, 0.14302775903357146), (235, 0.02159098535791548), (238, 0.018208081203039732)]\n",
      "[(36, 0.07807243442571236), (84, 0.2730703160513895), (139, 0.014247779546852593), (146, 0.03810961976753362), (181, 0.016593252557485314), (221, 0.17967213150761804), (227, 0.05234291623564771), (230, 0.022529974790510745)]\n",
      "[(11, 0.021329750470727996), (23, 0.02430212908242742), (38, 0.03364264434291798), (42, 0.04702791712536044), (46, 0.07460728608311425), (79, 0.02878753780088938), (84, 0.056292127942538225), (85, 0.17915642090557204), (207, 0.11357057109205972), (221, 0.05548110962650324)]\n",
      "[(18, 0.02027593776925161), (23, 0.11568525666243457), (85, 0.045811808158495566), (99, 0.021280773537311958), (116, 0.041298581013631125), (132, 0.04703439649812544), (139, 0.025704645899387243), (143, 0.03533920563281083), (153, 0.038442472503031934), (221, 0.3014264190832619), (245, 0.040576741149115546)]\n",
      "[(8, 0.026593333021452732), (23, 0.15926602350778427), (53, 0.022642617183808226), (84, 0.06453419262971816), (147, 0.014922579523978874), (173, 0.09393470678147461), (174, 0.021636046389904774), (184, 0.034274034453064595), (189, 0.011448130641609988), (191, 0.020309680992780226), (203, 0.017868545747353255), (205, 0.015498689109987373), (207, 0.014920886861403807), (220, 0.0258836492766014), (221, 0.10918639991470332), (225, 0.037860951024834985)]\n",
      "[(8, 0.06705897167573037), (9, 0.015351812969157568), (11, 0.01383372099201495), (23, 0.08902775281839433), (84, 0.22571268502052463), (103, 0.015112389610385118), (113, 0.015611723815648675), (114, 0.04843982927552318), (220, 0.011105962674273821), (221, 0.2402117522939484)]\n",
      "[(1, 0.03619525828213667), (22, 0.037965731643873644), (25, 0.055930675247612784), (80, 0.0376067015957314), (95, 0.02122779236870512), (111, 0.03161006387699284), (113, 0.033119998627022404), (114, 0.15725144037283262), (159, 0.023628870571081293), (212, 0.025709658296328514), (221, 0.17067417051701256), (224, 0.05064506408874434), (247, 0.020347829687733226)]\n",
      "[(8, 0.05446082799885819), (10, 0.018744416600460466), (22, 0.07338627823488582), (24, 0.12482900600506557), (79, 0.05892151617947738), (84, 0.0871784666134882), (113, 0.018990375351886402), (146, 0.05868224237852645), (220, 0.014845044800383709), (221, 0.19669954291006433), (231, 0.02311237552464271)]\n",
      "[(45, 0.012830187419499471), (84, 0.141527079256739), (93, 0.1611599291973817), (98, 0.06868894969231072), (105, 0.011488865202841392), (121, 0.031356126946451415), (141, 0.016748167778194854), (202, 0.015524245413640765), (220, 0.04394701146968537), (221, 0.1385687463240566), (230, 0.0324429438796638), (240, 0.010893796351918403), (247, 0.011121900228823684)]\n",
      "[(8, 0.03170036533400939), (11, 0.09564868377425981), (22, 0.10242991579566535), (23, 0.0625098735940985), (71, 0.020078937742955553), (84, 0.10730463892572287), (85, 0.07453176639311151), (103, 0.01641819211968184), (106, 0.036822676650112934), (116, 0.024964753030753177), (173, 0.01345695225494696), (207, 0.01945129861991175), (221, 0.13631669767781635)]\n",
      "[(25, 0.01753196462922142), (38, 0.022672271090683548), (56, 0.04962397977771953), (81, 0.016590034763874165), (84, 0.10243154068748743), (85, 0.06865880415130662), (132, 0.01610523290347643), (154, 0.04283385094378254), (182, 0.03397285150262758), (183, 0.022099151470814843), (186, 0.018413326421014304), (189, 0.06185652282516367), (207, 0.018280044759691763), (221, 0.21061617076815037), (226, 0.023728758469328325)]\n",
      "[(11, 0.01374097779956201), (23, 0.055513093826548246), (24, 0.013397331228974668), (57, 0.038455369613724595), (64, 0.055605156076331076), (70, 0.0540054962598897), (84, 0.10263142849705902), (89, 0.014659226216158307), (101, 0.01519084092161726), (103, 0.015689267994176664), (114, 0.023289387173152414), (116, 0.05976578747144588), (221, 0.247223004380722), (232, 0.015853388889639566)]\n",
      "[(23, 0.23942331769453765), (37, 0.02617041412366411), (44, 0.049874989325752676), (53, 0.02356294292681533), (57, 0.0259670248198019), (82, 0.017090982389960128), (90, 0.07133007750405984), (93, 0.03482715221455989), (106, 0.021340222848045615), (208, 0.014126657045111828), (220, 0.029100071326883394), (221, 0.13697659252492136)]\n",
      "[(23, 0.058858688206380655), (42, 0.018605219120098365), (56, 0.14040445932008588), (68, 0.017621885455364576), (81, 0.02892314780164218), (84, 0.089139494597331), (132, 0.014465626156369033), (168, 0.019462774662746114), (180, 0.020237466023107397), (197, 0.015379710192947982), (205, 0.017865018719337513), (212, 0.011256677569261053), (221, 0.19751901767559504), (244, 0.019306047746877376), (247, 0.015106656226545896)]\n",
      "[(16, 0.19729142903311445), (22, 0.01841363213829237), (37, 0.012894753056953754), (84, 0.1302995488545037), (85, 0.013680588576485485), (96, 0.04598613113848531), (132, 0.01848628408539422), (174, 0.039463367422798865), (216, 0.01535658347273769), (220, 0.020788759844832354), (221, 0.12395018638544189), (222, 0.0162826514011178), (232, 0.0301984018766193)]\n",
      "[(6, 0.014650824388429873), (11, 0.16826054006577312), (84, 0.01163134015212366), (85, 0.06423476421318229), (114, 0.1108394849293715), (152, 0.02060522962854722), (173, 0.052308890903815644), (221, 0.2835620634493351)]\n",
      "[(0, 0.028634912529064655), (8, 0.0722383758445283), (11, 0.053684592301485014), (24, 0.053144549504937), (84, 0.0198899351235427), (116, 0.0750137561844352), (146, 0.023951375391410647), (155, 0.02083736969786946), (220, 0.023621796924785526), (221, 0.3291896590163661), (236, 0.022791150160666566)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 0.017803993958233545), (74, 0.01694026525232577), (79, 0.015110626087177387), (84, 0.16727023361563456), (87, 0.04158751258792904), (91, 0.03600122076656171), (96, 0.0170532744149395), (132, 0.11380488050714208), (137, 0.015202335111634947), (142, 0.014202460400227358), (159, 0.019033585139078567), (172, 0.052567593948112716), (220, 0.04157683486635388), (221, 0.14229288867669937), (228, 0.01778643911258368)]\n",
      "[(8, 0.011994989553015225), (27, 0.016274762283589195), (58, 0.029366752098552048), (68, 0.09117561565541626), (75, 0.01863645658381619), (84, 0.1291667566108416), (85, 0.06563608786808864), (117, 0.08388524528767234), (132, 0.012928232869323196), (149, 0.013233434078265112), (169, 0.03273037492965544), (207, 0.013982435115639224), (220, 0.02765708089114214), (221, 0.1769737836754215), (246, 0.02345390646880134)]\n",
      "[(2, 0.010620010734590528), (5, 0.010000089691744585), (23, 0.058421930589432534), (26, 0.05019014022056031), (33, 0.02042573738551748), (61, 0.010824342892429993), (62, 0.01124483638716797), (84, 0.1072176029611599), (98, 0.01722616258523353), (100, 0.02096684222908267), (132, 0.06133269829805125), (134, 0.0316558485561806), (138, 0.024973998980991164), (148, 0.018992834739250782), (157, 0.03009621834449681), (165, 0.011680677064384687), (214, 0.022830352172917255), (221, 0.20310552013657057)]\n",
      "[(29, 0.017896898332665046), (64, 0.01460580531200312), (65, 0.0579144923453675), (84, 0.09411309169138497), (121, 0.1349935821934886), (125, 0.0848642647180661), (132, 0.04758862294402415), (143, 0.015082220084585376), (161, 0.025289540430983623), (170, 0.01464606474443784), (195, 0.01595344252780594), (220, 0.02624750043568526), (221, 0.14876882428992497), (222, 0.013565356931491971)]\n",
      "[(8, 0.07205334243379599), (14, 0.023074017881379874), (30, 0.047513163998745375), (32, 0.021255620668228985), (46, 0.025125679242408137), (67, 0.020828490988583175), (74, 0.0223475909649434), (84, 0.06067773521106933), (173, 0.0215790129004152), (176, 0.023130319545829222), (220, 0.0479956228946925), (221, 0.34454861381180835)]\n"
     ]
    }
   ],
   "source": [
    "test_abs=pd.read_pickle('test_abs')\n",
    "n=len(test_abs)\n",
    "ctr=0\n",
    "avg_precision=[0]*11\n",
    "valid_file=list(judge.keys())\n",
    "for i in range(n):\n",
    "    if i in valid_file:        \n",
    "        res=predict(test_abs['abstract'].iloc[i],reviewer_vec)\n",
    "\n",
    "        for k in range(1,11):\n",
    "            avg_precision[k]+=precision_k(res,judge,k,i)\n",
    "            \n",
    "        ctr+=1\n",
    "avg_precision=[ele/ctr for ele in avg_precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12705882352941178,\n",
       " 0.15676470588235294,\n",
       " 0.1469607843137255,\n",
       " 0.1322794117647059,\n",
       " 0.11952941176470591,\n",
       " 0.10612745098039214,\n",
       " 0.1049159663865546,\n",
       " 0.0967279411764706,\n",
       " 0.09140522875816995,\n",
       " 0.09305882352941176]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_abs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8d0a7263dd64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleantext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_abs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbow_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_abs' is not defined"
     ]
    }
   ],
   "source": [
    "clean_text=list(word_tokenize(cleantext(test_abs['abstract'].iloc[1]))) \n",
    "bow_text=dictionary.doc2bow(clean_text)\n",
    "t=tfidf[bow_text]\n",
    "lda_model[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07852941176470588,\n",
       " 0.11764705882352941,\n",
       " 0.13725490196078433,\n",
       " 0.12257352941176472,\n",
       " 0.11570588235294117,\n",
       " 0.10622549019607845,\n",
       " 0.09386554621848738,\n",
       " 0.09316176470588236,\n",
       " 0.0838888888888889,\n",
       " 0.08532352941176473]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vec_bow={}\n",
    "n=len(dictionary)\n",
    "for sno,name in enumerate(review_lt):\n",
    "    samp_df=data_df[data_df['author']==name]\n",
    "    index_lt=samp_df.index\n",
    "    wts=np.zeros([n], dtype = float)\n",
    "    ctr=0\n",
    "    #print(index_lt)\n",
    "    for ind in index_lt:\n",
    "        ctr+=1\n",
    "        text=samp_df['title'][ind]+' '+samp_df['abstract'][ind]\n",
    "        clean_text=list(word_tokenize(cleantext(text))) \n",
    "        bow_text=dictionary.doc2bow(clean_text)\n",
    "        t=tfidf[bow_text]\n",
    "        for ind,wt in t:\n",
    "            wts[ind]+=wt\n",
    "    wts=wts/ctr\n",
    "    reviewer_vec_bow[sno]=wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bow(text,reviewer_vec_bow):\n",
    "    clean_text=list(word_tokenize(cleantext(text))) \n",
    "    bow_text=dictionary.doc2bow(clean_text)\n",
    "    tfidf_text=tfidf[bow_text]\n",
    "    \n",
    "    n=len(dictionary)\n",
    "\n",
    "    bow_vec=np.zeros(n, dtype = float)\n",
    "    for ind,wt in tfidf_text:\n",
    "        bow_vec[ind]=wt\n",
    "        \n",
    "    #print(topic)\n",
    "    \n",
    "    reviewer_scr=[]\n",
    "    index_lt=list(reviewer_vec_bow.keys())\n",
    "    for index in index_lt:\n",
    "        val=1-cosine(bow_vec,reviewer_vec_bow[index])\n",
    "        if np.isnan(val):\n",
    "            val=0\n",
    "        reviewer_scr.append([index+1,val])\n",
    "        \n",
    "    reviewer_scr.sort(key=lambda x: x[1],reverse=True)\n",
    "    \n",
    "    pred=[]\n",
    "    for i in range(10):\n",
    "        pred.append(reviewer_scr[i][0])\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abs=pd.read_pickle('test_abs')\n",
    "n=len(test_abs)\n",
    "ctr=0\n",
    "avg_precision=[0]*11\n",
    "valid_file=list(judge.keys())\n",
    "for i in range(n):\n",
    "    if i in valid_file:        \n",
    "        res=predict_bow(test_abs['abstract'].iloc[i],reviewer_vec_bow)\n",
    "        #print(res,i)\n",
    "        #print('////////////')\n",
    "        for k in range(1,11):\n",
    "            avg_precision[k]+=precision_k(res,judge,k,i)\n",
    "            #print(precision_k(res,judge,k,i),i)\n",
    "        ctr+=1\n",
    "avg_precision=[ele/ctr for ele in avg_precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47058823529411764,\n",
       " 0.39705882352941174,\n",
       " 0.3529411764705882,\n",
       " 0.35294117647058826,\n",
       " 0.35294117647058815,\n",
       " 0.3333333333333333,\n",
       " 0.3025210084033614,\n",
       " 0.2867647058823529,\n",
       " 0.2712418300653595,\n",
       " 0.25882352941176473]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[234, 9, 53, 108, 224, 251, 197, 23, 235, 225]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_bow(test_abs['abstract'].iloc[103],reviewer_vec_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''path1=cwd+sym+'Saved_Items'+sym\n",
    "with open(path1+'reviewer_vec_bow.pkl','wb') as f:\n",
    "    pickle.dump(reviewer_vec_bow,f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewer_404(res,ground_tht,paper_no):\n",
    "    actual=ground_tht[paper_no]\n",
    "    actual_auth=[i[0] for i in actual]\n",
    "    rating=[i[1] for i in actual]\n",
    "    n=len(actual)\n",
    "    output=[]\n",
    "    for auth in res:\n",
    "        if auth not in actual_auth:\n",
    "            sim_scr=[0]*4\n",
    "            count=[0]*4\n",
    "            vec1=reviewer_vec_bow[auth-1]\n",
    "            for i in range(n):\n",
    "                val=1-cosine(vec1,reviewer_vec_bow[actual_auth[i]-1])\n",
    "                if np.isnan(val):\n",
    "                    val=0\n",
    "                sim_scr[rating[i]]+=val\n",
    "                count[rating[i]]+=1\n",
    "            sim_scr=[sim_scr[i]/count[i] for i in range(4) if count[i]]\n",
    "            max_scr=max(sim_scr)\n",
    "            ind=sim_scr.index(max_scr)\n",
    "            output.append([auth,ind])\n",
    "            #print(sim_scr)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k_404(res,gnd_tht,k,paper_no):\n",
    "    actual=gnd_tht[paper_no]\n",
    "    actual_auth=[i[0] for i in actual]\n",
    "    rating=[i[1] for i in actual]\n",
    "    n=len(actual)\n",
    "    soft_p=0\n",
    "    for i in range(k):\n",
    "        pred=res[i]\n",
    "        for j in range(n):\n",
    "            if actual_auth[j]==pred:\n",
    "                if rating[j]==3:\n",
    "                    soft_p+=1\n",
    "                elif rating[j]==2:\n",
    "                    soft_p+=0.67\n",
    "                elif rating[j]==1:\n",
    "                    soft_p+=0.33\n",
    "                else:\n",
    "                    break\n",
    "    rating_gen=reviewer_404(res,gnd_tht,paper_no)\n",
    "    auth_gen=[i[0] for i in rating_gen]\n",
    "    rating_gen=[i[1] for i in rating_gen]\n",
    "    n1=len(auth_gen)\n",
    "    for i in range(k):\n",
    "        pred=res[i]\n",
    "        for j in range(n1):\n",
    "            if auth_gen[j]==pred:\n",
    "                if rating_gen[j]==3:\n",
    "                    soft_p+=1\n",
    "                elif rating_gen[j]==2:\n",
    "                    soft_p+=0.67\n",
    "                elif rating_gen[j]==1:\n",
    "                    soft_p+=0.33\n",
    "                else:\n",
    "                    break\n",
    "    soft_p=soft_p/k\n",
    "    return soft_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def diversity(res):\n",
    "    div=0\n",
    "    n=len(res)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            sim=cosine_similarity([reviewer_vec[res[i]-1],reviewer_vec[res[j]-1]])\n",
    "            sim=sim[0][1]\n",
    "            div+=1-sim\n",
    "    div=div/(n*(n-1))\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(p,q):\n",
    "    \"\"\" Epsilon is used here to avoid conditional code for\n",
    "    checking that neither P nor Q is equal to 0. \"\"\"\n",
    "    epsilon = 0.00001\n",
    "    \n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    vec1=reviewer_vec[p-1]\n",
    "    vec2=reviewer_vec[q-1]\n",
    "    \n",
    "    vec1=np.array(vec1)\n",
    "    v1sum=np.sum(vec1)\n",
    "    vec2=np.array(vec2)\n",
    "    v2sum=np.sum(vec2)\n",
    "\n",
    "    vec1=np.divide(vec1,v1sum)\n",
    "    vec2=np.divide(vec2,v2sum)\n",
    "        \n",
    "    vec1 = vec1+epsilon\n",
    "    vec2 = vec2+epsilon\n",
    "    \n",
    "    if v1sum==0 or v2sum==0:\n",
    "        return 0\n",
    "\n",
    "    divergence = np.sum(vec1*np.log10(vec1/vec2))\n",
    "    return divergence\n",
    "\n",
    "def diversity(res):\n",
    "    div=0\n",
    "    n=len(res)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            div+=KL(res[i],res[j])\n",
    "    div=div/(n*(n-1))\n",
    "    return div\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k_404(res,gnd_tht,k,paper_no):\n",
    "    actual=gnd_tht[paper_no]\n",
    "    actual_auth=[i[0] for i in actual]\n",
    "    rating=[i[1] for i in actual]\n",
    "    n=len(actual)\n",
    "    soft_p=0\n",
    "    for i in range(k):\n",
    "        pred=res[i]\n",
    "        for j in range(n):\n",
    "            if actual_auth[j]==pred:\n",
    "                if rating[j]==3:\n",
    "                    soft_p+=1\n",
    "                elif rating[j]==2:\n",
    "                    soft_p+=0.67\n",
    "                elif rating[j]==1:\n",
    "                    soft_p+=0.33\n",
    "                else:\n",
    "                    break\n",
    "    rating_gen=reviewer_404(res,gnd_tht,paper_no)\n",
    "    auth_gen=[i[0] for i in rating_gen]\n",
    "    rating_gen=[i[1] for i in rating_gen]\n",
    "    n1=len(auth_gen)\n",
    "    for i in range(k):\n",
    "        pred=res[i]\n",
    "        for j in range(n1):\n",
    "            if auth_gen[j]==pred:\n",
    "                if rating_gen[j]==3:\n",
    "                    soft_p+=1\n",
    "                elif rating_gen[j]==2:\n",
    "                    soft_p+=0.67\n",
    "                elif rating_gen[j]==1:\n",
    "                    soft_p+=0.33\n",
    "                else:\n",
    "                    break\n",
    "    soft_p=soft_p/k\n",
    "    return soft_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abs=pd.read_pickle('test_abs')\n",
    "n=len(test_abs)\n",
    "ctr=0\n",
    "avg_precision=[0]*11\n",
    "valid_file=list(judge.keys())\n",
    "for i in range(n):\n",
    "    if i in valid_file:        \n",
    "        res=predict(test_abs['abstract'].iloc[i],reviewer_vec_blda)\n",
    "        #print(res)\n",
    "        for k in range(1,11):\n",
    "            avg_precision[k]+=precision_k(res,judge,k,i)\n",
    "        ctr+=1\n",
    "avg_precision=[ele/ctr for ele in avg_precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17647058823529413,\n",
       " 0.16176470588235295,\n",
       " 0.1372549019607843,\n",
       " 0.1323529411764706,\n",
       " 0.13529411764705884,\n",
       " 0.1323529411764706,\n",
       " 0.1260504201680672,\n",
       " 0.125,\n",
       " 0.12418300653594774,\n",
       " 0.11764705882352944]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
